{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGilfsGcK-EQ",
        "outputId": "1c3adf0a-4d7e-4352-dcb3-28c022b9aa63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxufDXuQDqTD"
      },
      "source": [
        "## Decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ji-IWUgbDr6Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DeepLabHeadV3Plus(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels=768, low_level_channels=96,\n",
        "                 aspp_dilate=[12, 24, 36]):\n",
        "        super(DeepLabHeadV3Plus, self).__init__()\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.aspp = ASPP(in_channels, aspp_dilate)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(304, 256, 3, padding=1, bias=False),  # Todo change accordingly to original image shape\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.1),  # added for fun\n",
        "            nn.Conv2d(256, num_classes, 1)\n",
        "        )\n",
        "        self._init_weight()\n",
        "\n",
        "    def forward(self, low_features, out):\n",
        "        low_level_feature = self.project(low_features)\n",
        "        output_feature = self.aspp(out)\n",
        "        output_feature = F.interpolate(output_feature, size=low_level_feature.shape[2:], mode='bilinear',\n",
        "                                       align_corners=False)\n",
        "        return self.classifier(torch.cat([low_level_feature, output_feature], dim=1))\n",
        "\n",
        "    def _init_weight(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class ASPPConv(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, dilation):\n",
        "        modules = [\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "        super(ASPPConv, self).__init__(*modules)\n",
        "\n",
        "\n",
        "class ASPPPooling(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ASPPPooling, self).__init__(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),  # Todo error\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.shape[-2:]\n",
        "        x = super(ASPPPooling, self).forward(x)\n",
        "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, atrous_rates):\n",
        "        super(ASPP, self).__init__()\n",
        "        out_channels = 256\n",
        "        modules = []\n",
        "        modules.append(nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)))\n",
        "\n",
        "        rate1, rate2, rate3 = tuple(atrous_rates)\n",
        "        modules.append(ASPPConv(in_channels, out_channels, rate1))\n",
        "        modules.append(ASPPConv(in_channels, out_channels, rate2))\n",
        "        modules.append(ASPPConv(in_channels, out_channels, rate3))\n",
        "        modules.append(ASPPPooling(in_channels, out_channels))\n",
        "\n",
        "        self.convs = nn.ModuleList(modules)\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.1), )\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = []\n",
        "        for conv in self.convs:\n",
        "            res.append(conv(x))\n",
        "        res = torch.cat(res, dim=1)\n",
        "        return self.project(res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQgw_b2RDefs"
      },
      "source": [
        "## Teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uVnH3yRnDj8R"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import OneFormerForUniversalSegmentation\n",
        "\n",
        "\n",
        "class SwinDeepLabV3Plus(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SwinDeepLabV3Plus, self).__init__()\n",
        "\n",
        "        # Swin Transformer as backbone - (pre-trained backbone)\n",
        "        oneformer_swin_t = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n",
        "        self.backbone = oneformer_swin_t.model.pixel_level_module.encoder\n",
        "\n",
        "        # DeepLabv3+ as classifier\n",
        "        self.classifier = DeepLabHeadV3Plus(num_classes=133)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through Swin Transformer\n",
        "        f_maps = self.backbone(x)\n",
        "        features = f_maps['feature_maps']\n",
        "        low_features = features[0]\n",
        "        out_backbone = features[3]\n",
        "\n",
        "        # Forward pass through DeepLabV3+\n",
        "        logits_output = self.classifier(low_features, out_backbone)\n",
        "\n",
        "        preds = logits_output.softmax(dim=1)\n",
        "        preds = preds.argmax(dim=1)\n",
        "\n",
        "        return logits_output, preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "N4ewdzQjCQN-"
      },
      "source": [
        "## Prepare dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "CE6GcwTACQOB"
      },
      "outputs": [],
      "source": [
        "# from utils.prepare_dataset import prepare_data\n",
        "\n",
        "# prepare_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnNsJcweESkS"
      },
      "source": [
        "## Visualization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "leHQ47PzEYBi"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "    0: \"person\",\n",
        "    1: \"bicycle\",\n",
        "    2: \"car\",\n",
        "    3: \"motorcycle\",\n",
        "    4: \"airplane\",\n",
        "    5: \"bus\",\n",
        "    6: \"train\",\n",
        "    7: \"truck\",\n",
        "    8: \"boat\",\n",
        "    9: \"traffic light\",\n",
        "    10: \"fire hydrant\",\n",
        "    11: \"stop sign\",\n",
        "    12: \"parking meter\",\n",
        "    13: \"bench\",\n",
        "    14: \"bird\",\n",
        "    15: \"cat\",\n",
        "    16: \"dog\",\n",
        "    17: \"horse\",\n",
        "    18: \"sheep\",\n",
        "    19: \"cow\",\n",
        "    20: \"elephant\",\n",
        "    21: \"bear\",\n",
        "    22: \"zebra\",\n",
        "    23: \"giraffe\",\n",
        "    24: \"backpack\",\n",
        "    25: \"umbrella\",\n",
        "    26: \"handbag\",\n",
        "    27: \"tie\",\n",
        "    28: \"suitcase\",\n",
        "    29: \"frisbee\",\n",
        "    30: \"skis\",\n",
        "    31: \"snowboard\",\n",
        "    32: \"sports ball\",\n",
        "    33: \"kite\",\n",
        "    34: \"baseball bat\",\n",
        "    35: \"baseball glove\",\n",
        "    36: \"skateboard\",\n",
        "    37: \"surfboard\",\n",
        "    38: \"tennis racket\",\n",
        "    39: \"bottle\",\n",
        "    40: \"wine glass\",\n",
        "    41: \"cup\",\n",
        "    42: \"fork\",\n",
        "    43: \"knife\",\n",
        "    44: \"spoon\",\n",
        "    45: \"bowl\",\n",
        "    46: \"banana\",\n",
        "    47: \"apple\",\n",
        "    48: \"sandwich\",\n",
        "    49: \"orange\",\n",
        "    50: \"broccoli\",\n",
        "    51: \"carrot\",\n",
        "    52: \"hot dog\",\n",
        "    53: \"pizza\",\n",
        "    54: \"donut\",\n",
        "    55: \"cake\",\n",
        "    56: \"chair\",\n",
        "    57: \"couch\",\n",
        "    58: \"potted plant\",\n",
        "    59: \"bed\",\n",
        "    60: \"dining table\",\n",
        "    61: \"toilet\",\n",
        "    62: \"tv\",\n",
        "    63: \"laptop\",\n",
        "    64: \"mouse\",\n",
        "    65: \"remote\",\n",
        "    66: \"keyboard\",\n",
        "    67: \"cell phone\",\n",
        "    68: \"microwave\",\n",
        "    69: \"oven\",\n",
        "    70: \"toaster\",\n",
        "    71: \"sink\",\n",
        "    72: \"refrigerator\",\n",
        "    73: \"book\",\n",
        "    74: \"clock\",\n",
        "    75: \"vase\",\n",
        "    76: \"scissors\",\n",
        "    77: \"teddy bear\",\n",
        "    78: \"hair drier\",\n",
        "    79: \"toothbrush\",\n",
        "    80: \"banner\",\n",
        "    81: \"blanket\",\n",
        "    82: \"bridge\",\n",
        "    83: \"cardboard\",\n",
        "    84: \"counter\",\n",
        "    85: \"curtain\",\n",
        "    86: \"door-stuff\",\n",
        "    87: \"floor-wood\",\n",
        "    88: \"flower\",\n",
        "    89: \"fruit\",\n",
        "    90: \"gravel\",\n",
        "    91: \"house\",\n",
        "    92: \"light\",\n",
        "    93: \"mirror-stuff\",\n",
        "    94: \"net\",\n",
        "    95: \"pillow\",\n",
        "    96: \"platform\",\n",
        "    97: \"playingfield\",\n",
        "    98: \"railroad\",\n",
        "    99: \"river\",\n",
        "    100: \"road\",\n",
        "    101: \"roof\",\n",
        "    102: \"sand\",\n",
        "    103: \"sea\",\n",
        "    104: \"shelf\",\n",
        "    105: \"snow\",\n",
        "    106: \"stairs\",\n",
        "    107: \"tent\",\n",
        "    108: \"towel\",\n",
        "    109: \"wall-brick\",\n",
        "    110: \"wall-stone\",\n",
        "    111: \"wall-tile\",\n",
        "    112: \"wall-wood\",\n",
        "    113: \"water-other\",\n",
        "    114: \"window-blind\",\n",
        "    115: \"window-other\",\n",
        "    116: \"tree-merged\",\n",
        "    117: \"fence-merged\",\n",
        "    118: \"ceiling-merged\",\n",
        "    119: \"sky-other-merged\",\n",
        "    120: \"cabinet-merged\",\n",
        "    121: \"table-merged\",\n",
        "    122: \"floor-other-merged\",\n",
        "    123: \"pavement-merged\",\n",
        "    124: \"mountain-merged\",\n",
        "    125: \"grass-merged\",\n",
        "    126: \"dirt-merged\",\n",
        "    127: \"paper-merged\",\n",
        "    128: \"food-other-merged\",\n",
        "    129: \"building-other-merged\",\n",
        "    130: \"rock-merged\",\n",
        "    131: \"wall-other-merged\",\n",
        "    132: \"rug-merged\"\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tXIU5vJeEU-D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import cm\n",
        "\n",
        "\n",
        "def visualize_segmentation(segmentation_tensor, plot_save_path):\n",
        "    # todo le labels vengono tagliate a destra e in basso\n",
        "    # get all the unique numbers\n",
        "    labels_ids = torch.unique(segmentation_tensor).tolist()\n",
        "    print(labels_ids)\n",
        "\n",
        "    # Map ids with RGB colors\n",
        "    coco_color_map = {id: cm.viridis(index / len(labels_ids)) for index, id in enumerate(labels_ids)}\n",
        "\n",
        "    # Map the class indices to RGB colors using NumPy vectorized operations\n",
        "    segmented_image = np.zeros((segmentation_tensor.shape[0], segmentation_tensor.shape[1], 4), dtype=np.float32)\n",
        "    class_indices = segmentation_tensor.long().cpu().numpy()\n",
        "\n",
        "    mask = np.isin(class_indices, list(coco_color_map.keys()))\n",
        "    segmented_image[mask] = [coco_color_map[class_index] for class_index in class_indices[mask]]\n",
        "\n",
        "    # Create legend labels based on id2label mapping\n",
        "    legend_labels = [id2label[class_id] for class_id in labels_ids]\n",
        "\n",
        "    # Display the segmented image with legend\n",
        "    plt.imshow(segmented_image)\n",
        "    plt.axis('off')\n",
        "    plt.title('Segmentation Map')\n",
        "\n",
        "    # Adjust layout to prevent overlapping\n",
        "    plt.tight_layout()\n",
        "\n",
        "    handles = [mpatches.Patch(color=coco_color_map[label_id], label=id2label[label_id]) for label_id in labels_ids]\n",
        "\n",
        "    # Create legend with class labels\n",
        "    plt.legend(handles=handles, labels=legend_labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
        "    plt.savefig('/content/drive/MyDrive/' + plot_save_path)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8digwYRECl9"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x3qL-Vf9EDnL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "from typing import Optional\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "# from .functional import soft_dice_score\n",
        "\n",
        "__all__ = [\"DiceLoss\"]\n",
        "\n",
        "\n",
        "def soft_dice_score(output: torch.Tensor, target: torch.Tensor, smooth: float = 0.0,\n",
        "                    eps: float = 1e-7, dims=None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "\n",
        "    :param output:\n",
        "    :param target:\n",
        "    :param smooth:\n",
        "    :param eps:\n",
        "    :return:\n",
        "\n",
        "    Shape:\n",
        "        - Input: :math:`(N, NC, *)` where :math:`*` means any number\n",
        "            of additional dimensions\n",
        "        - Target: :math:`(N, NC, *)`, same shape as the input\n",
        "        - Output: scalar.\n",
        "\n",
        "    \"\"\"\n",
        "    assert output.size() == target.size()\n",
        "    if dims is not None:\n",
        "        intersection = torch.sum(output * target, dim=dims)\n",
        "        cardinality = torch.sum(output + target, dim=dims)\n",
        "    else:\n",
        "        intersection = torch.sum(output * target)\n",
        "        cardinality = torch.sum(output + target)\n",
        "    dice_score = (2.0 * intersection + smooth) / (cardinality + smooth).clamp_min(eps)\n",
        "    return dice_score\n",
        "\n",
        "\n",
        "class DiceLoss(_Loss):\n",
        "    def __init__(\n",
        "            self,\n",
        "            log_loss=False,\n",
        "            from_logits=True,\n",
        "            smooth: float = 1e-7,\n",
        "            ignore_index=None,\n",
        "            eps=1e-7,\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        :param log_loss: If True, loss computed as `-log(jaccard)`; otherwise `1 - jaccard`\n",
        "        :param from_logits: If True assumes input is raw logits\n",
        "        :param smooth:\n",
        "        :param ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n",
        "        :param eps: Small epsilon for numerical stability\n",
        "        \"\"\"\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "        self.from_logits = from_logits\n",
        "        self.smooth = smooth\n",
        "        self.eps = eps\n",
        "        self.log_loss = log_loss\n",
        "\n",
        "    def forward(self, y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "\n",
        "        :param y_pred: NxCxHxW\n",
        "        :param y_true: NxHxW\n",
        "        :return: scalar\n",
        "        \"\"\"\n",
        "        assert y_true.size(0) == y_pred.size(0)\n",
        "\n",
        "        if self.from_logits:\n",
        "            # Apply activations to get [0..1] class probabilities\n",
        "            # Log-Exp gives more stable result and does not cause vanishing gradient on extreme values 0 and 1\n",
        "            y_pred = y_pred.log_softmax(dim=1).exp()\n",
        "\n",
        "        bs = y_true.size(0)\n",
        "        num_classes = y_pred.size(1)\n",
        "        dims = (0, 2)\n",
        "\n",
        "        y_true = y_true.view(bs, -1)\n",
        "        y_pred = y_pred.view(bs, num_classes, -1)\n",
        "\n",
        "        y_true = F.one_hot(y_true, num_classes)  # N,H*W -> N,H*W, C\n",
        "        y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n",
        "\n",
        "        scores = soft_dice_score(y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims)\n",
        "\n",
        "        if self.log_loss:\n",
        "            loss = -torch.log(scores.clamp_min(self.eps))\n",
        "        else:\n",
        "            loss = 1.0 - scores\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "\n",
        "def softmax_focal_loss_with_logits(\n",
        "    output: torch.Tensor,\n",
        "    target: torch.Tensor,\n",
        "    gamma: float = 2.0,\n",
        "    reduction: str = \"mean\",\n",
        "    normalized: bool = False,\n",
        "    reduced_threshold: Optional[float] = None,\n",
        "    eps: float = 1e-6,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Softmax version of focal loss between target and output logits.\n",
        "    See :class:`~pytorch_toolbelt.losses.FocalLoss` for details.\n",
        "\n",
        "    Args:\n",
        "        output: Tensor of shape [B, C, *] (Similar to nn.CrossEntropyLoss)\n",
        "        target: Tensor of shape [B, *] (Similar to nn.CrossEntropyLoss)\n",
        "        gamma: Focal loss power factor\n",
        "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
        "            'none' | 'mean' | 'sum' | 'batchwise_mean'. 'none': no reduction will be applied,\n",
        "            'mean': the sum of the output will be divided by the number of\n",
        "            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
        "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
        "            specifying either of those two args will override :attr:`reduction`.\n",
        "            'batchwise_mean' computes mean loss per sample in batch. Default: 'mean'\n",
        "        normalized (bool): Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n",
        "        reduced_threshold (float, optional): Compute reduced focal loss (https://arxiv.org/abs/1903.01347).\n",
        "    \"\"\"\n",
        "    log_softmax = F.log_softmax(output, dim=1)\n",
        "\n",
        "    loss = F.nll_loss(log_softmax, target, reduction=\"none\")\n",
        "    pt = torch.exp(-loss)\n",
        "\n",
        "    # compute the loss\n",
        "    if reduced_threshold is None:\n",
        "        focal_term = (1.0 - pt).pow(gamma)\n",
        "    else:\n",
        "        focal_term = ((1.0 - pt) / reduced_threshold).pow(gamma)\n",
        "        focal_term[pt < reduced_threshold] = 1\n",
        "\n",
        "    loss = focal_term * loss\n",
        "\n",
        "    if normalized:\n",
        "        norm_factor = focal_term.sum().clamp_min(eps)\n",
        "        loss = loss / norm_factor\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    if reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "    if reduction == \"batchwise_mean\":\n",
        "        loss = loss.sum(0)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "class CrossEntropyFocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal loss for multi-class problem. It uses softmax to compute focal term instead of sigmoid as in\n",
        "    original paper. This loss expects target labes to have one dimension less (like in nn.CrossEntropyLoss).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        gamma: float = 2.0,\n",
        "        reduction: str = \"mean\",\n",
        "        normalized: bool = False,\n",
        "        reduced_threshold: Optional[float] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        :param alpha:\n",
        "        :param gamma:\n",
        "        :param ignore_index: If not None, targets with given index are ignored\n",
        "        :param reduced_threshold: A threshold factor for computing reduced focal loss\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.reduced_threshold = reduced_threshold\n",
        "        self.normalized = normalized\n",
        "\n",
        "    def forward(self, inputs: Tensor, targets: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            inputs: [B,C,H,W] tensor\n",
        "            targets: [B,H,W] tensor\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        return softmax_focal_loss_with_logits(\n",
        "            inputs,\n",
        "            targets,\n",
        "            gamma=self.gamma,\n",
        "            reduction=self.reduction,\n",
        "            normalized=self.normalized,\n",
        "            reduced_threshold=self.reduced_threshold,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1TwAO1YaCQOD"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z_o6FW89EljE"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from transformers import OneFormerProcessor, AutoModelForUniversalSegmentation\n",
        "\n",
        "\n",
        "def teacher_forward(teacher, **inputs):\n",
        "    raw_out = teacher(**inputs)\n",
        "\n",
        "    class_queries_logits = raw_out.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n",
        "    masks_queries_logits = raw_out.masks_queries_logits  # [batch_size, num_queries, height, width]\n",
        "\n",
        "    # Remove the null class `[..., :-1]`\n",
        "    masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n",
        "    masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n",
        "\n",
        "    # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n",
        "    segmentation_logits = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)  # not probability\n",
        "    # print(\"sum: \", torch.sum(segmentation_logits[:, :, 0, 0], dim=1))\n",
        "    segmentation_logits = F.interpolate(segmentation_logits, size=(128, 128),\n",
        "                                        mode='bilinear', align_corners=False)\n",
        "\n",
        "    semantic_segmentation = segmentation_logits.softmax(dim=1)\n",
        "\n",
        "    semantic_segmentation = semantic_segmentation.argmax(dim=1)\n",
        "    # print(\"segmentation_logits: \", segmentation_logits.shape)\n",
        "    # print(\"semantic_segmentation: \", semantic_segmentation.shape)\n",
        "    return segmentation_logits, semantic_segmentation\n",
        "\n",
        "\n",
        "def train(stud_id, path_to_save_model=None):\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    # print(torch.cuda.is_available())\n",
        "    print('Device: ', device)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    processor_teacher = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_coco_swin_large\")\n",
        "    teacher = AutoModelForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_coco_swin_large\").to(device)\n",
        "\n",
        "    # processor_student = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\") # inutile\n",
        "    student = SwinDeepLabV3Plus(num_classes=133).to(device)\n",
        "\n",
        "    # freezing backbone's parameters\n",
        "    for param in student.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Load dataloaders\n",
        "    train_dl = torch.load(f'/content/drive/MyDrive/data_loaders/Train_dl_{stud_id}.pt')\n",
        "    val_dl = torch.load(f'/content/drive/MyDrive/data_loaders/Validation_dl_{stud_id}.pt')\n",
        "\n",
        "    # Hyper-params settings # todo change\n",
        "    learning_rate = 1e-03  # learning rate\n",
        "    milestones = [5, 10, 15]  # the epochs after which the learning rate is adjusted by gamma\n",
        "    gamma = 0.1  # gamma correction to the learning rate, after reaching the milestone epochs\n",
        "    weight_decay = 1e-05  # weight decay (L2 penalty)\n",
        "    epochs = 20\n",
        "    T = 2  # Temperature\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    use_scheduler = True  # use MultiStepLR scheduler\n",
        "    if use_scheduler:\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "\n",
        "    # Weights sum up to 1\n",
        "    # TODO paper 2015 dice che deve essere più alto al resto... online è sempre più basso del resto (tipo regularization)\n",
        "    kl_loss_weight = 0.8\n",
        "    ce_loss_weight = 0.1\n",
        "    dice_loss_weight = 0.1\n",
        "\n",
        "    ce_loss = nn.CrossEntropyLoss()  # todo usiamo focal al posto suo?\n",
        "    # ce_loss = CrossEntropyFocalLoss()  # focal loss\n",
        "    dice_loss = DiceLoss()\n",
        "\n",
        "    # Todo training loop\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    kl_l = []\n",
        "    dice_l = []\n",
        "    focal_l = []\n",
        "\n",
        "    # TODO add running loss and fix training loop\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        student.train()\n",
        "        running_loss = 0\n",
        "        running_kl = 0\n",
        "        running_dice = 0\n",
        "        running_focal = 0\n",
        "        n = 0\n",
        "        for i, (images, _) in enumerate(train_dl):\n",
        "            print(\"# epoch: \", epoch, \" - i: \", i)\n",
        "            batch_size = images.shape[0]\n",
        "            n += batch_size\n",
        "\n",
        "            semantic_inputs = processor_teacher(images=images, task_inputs=[\"semantic\"], return_tensors=\"pt\",\n",
        "                                                do_rescale=False).to(device)\n",
        "            semantic_inputs[\"task_inputs\"] = semantic_inputs[\"task_inputs\"].repeat(batch_size, 1)\n",
        "            # print(\"pixel_values: \", semantic_inputs['pixel_values'].shape)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass of the teacher model - do not save gradients to not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_logits, pseudo_labels = teacher_forward(teacher, **semantic_inputs)\n",
        "            # print(\"teacher_logits: \", teacher_logits.shape)\n",
        "            # print(\"pseudo_labels: \", pseudo_labels.shape)\n",
        "\n",
        "            student_input = F.interpolate(semantic_inputs[\"pixel_values\"], size=(512, 512),\n",
        "                                          mode='bilinear', align_corners=False)\n",
        "            # print(\"student_input: \", student_input.shape)\n",
        "\n",
        "            # Forward pass of the student model\n",
        "            student_logits, preds = student(student_input)\n",
        "            # print(\"student_logits: \", student_logits.shape)  # not probability\n",
        "            # print(\"preds: \", preds.shape)\n",
        "\n",
        "            # Soften the student logits by applying softmax first and log() second\n",
        "            soft_targets = F.softmax(teacher_logits / T, dim=1)\n",
        "            soft_prob = F.log_softmax(student_logits / T, dim=1)\n",
        "\n",
        "            # [batch * width * height, classes]\n",
        "            soft_targets = soft_targets.permute(0, 2, 3, 1).reshape(-1, 133)\n",
        "            soft_prob = soft_prob.permute(0, 2, 3, 1).reshape(-1, 133)\n",
        "\n",
        "            # Calculate the soft targets loss. [\"Distilling the knowledge in a neural network\"]\n",
        "            kl_div_res = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (T ** 2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            ce_res = ce_loss(student_logits, pseudo_labels)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            dice_res = dice_loss(student_logits, pseudo_labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = kl_loss_weight * kl_div_res + ce_loss_weight * ce_res + dice_loss_weight * dice_res\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            running_loss += loss * batch_size\n",
        "            running_kl += kl_loss_weight * kl_div_res * batch_size\n",
        "            running_dice += ce_loss_weight * ce_res * batch_size\n",
        "            running_focal += dice_loss_weight * dice_res * batch_size\n",
        "\n",
        "        train_loss.append(running_loss.detach().cpu() / n)\n",
        "        kl_l.append(running_kl.detach().cpu() / n)\n",
        "        dice_l.append(running_dice.detach().cpu() / n)\n",
        "        focal_l.append(running_focal.detach().cpu() / n)\n",
        "\n",
        "        # Todo validation: compute metrics\n",
        "        student.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0\n",
        "            n = 0\n",
        "            for i, (images, _) in enumerate(val_dl):\n",
        "                batch_size = images.shape[0]\n",
        "                n += batch_size\n",
        "\n",
        "                semantic_inputs = processor_teacher(images=images, task_inputs=[\"semantic\"], return_tensors=\"pt\",\n",
        "                                                    do_rescale=False).to(device)\n",
        "                semantic_inputs[\"task_inputs\"] = semantic_inputs[\"task_inputs\"].repeat(batch_size, 1)\n",
        "                # print(\"pixel_values: \", semantic_inputs['pixel_values'].shape)\n",
        "\n",
        "                teacher_logits, pseudo_labels = teacher_forward(teacher, **semantic_inputs)\n",
        "\n",
        "                student_logits, preds = student(semantic_inputs[\"pixel_values\"])\n",
        "\n",
        "                soft_targets = F.softmax(teacher_logits / T, dim=1)\n",
        "                soft_prob = F.log_softmax(student_logits / T, dim=1)\n",
        "\n",
        "                # [batch * width * height, classes]\n",
        "                soft_targets = soft_targets.permute(0, 2, 3, 1).reshape(-1, 133)\n",
        "                soft_prob = soft_prob.permute(0, 2, 3, 1).reshape(-1, 133)\n",
        "\n",
        "                kl_div_res = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (T ** 2)\n",
        "                ce_res = ce_loss(student_logits, pseudo_labels)\n",
        "                dice_res = dice_loss(student_logits, pseudo_labels)\n",
        "\n",
        "                loss = kl_loss_weight * kl_div_res + ce_loss_weight * ce_res + dice_loss_weight * dice_res\n",
        "\n",
        "                running_loss += loss * batch_size\n",
        "\n",
        "            val_loss.append(running_loss.detach().cpu() / n)\n",
        "\n",
        "        if use_scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Save and plot\n",
        "        if (epoch + 1) % 10 == 0:  # todo %10\n",
        "\n",
        "            if path_to_save_model is not None:\n",
        "                checkpoint_path = path_to_save_model + f'student_ckpt_epoch_{epoch + 1}.pth'\n",
        "                torch.save(student.state_dict(), checkpoint_path)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(epoch + 1), train_loss, label='Training Loss', marker='o')\n",
        "            plt.plot(range(epoch + 1), val_loss, label='Validation Loss', marker='o')\n",
        "\n",
        "            plt.plot(range(epoch + 1), kl_l, label='KL Loss', marker='o')\n",
        "            plt.plot(range(epoch + 1), dice_l, label='Dice Loss', marker='o')\n",
        "            plt.plot(range(epoch + 1), focal_l, label='Focal Loss', marker='o')\n",
        "\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Losses')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "\n",
        "            plot_path = f'epoch_{epoch}_lr_{learning_rate}_temp_{T}.png'\n",
        "\n",
        "            plt.savefig('/content/drive/MyDrive/' + plot_path)\n",
        "\n",
        "            # Save the loss plot\n",
        "            '''loss_plot_path = path_to_save_model + f'loss_plot_epoch_{epoch + 1}.png'\n",
        "            plt.savefig(loss_plot_path)'''\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            visualize_segmentation(pseudo_labels[0], plot_save_path= 'pseudo_labels_0_' + plot_path)\n",
        "            visualize_segmentation(pseudo_labels[1], plot_save_path= 'pseudo_labels_1_' + plot_path)\n",
        "\n",
        "            visualize_segmentation(preds[0], plot_save_path= 'pred_0_' + plot_path)\n",
        "            visualize_segmentation(preds[1], plot_save_path= 'pred_1_' + plot_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDUNCdDICQOD"
      },
      "outputs": [],
      "source": [
        "print(\"### Starting training... ###\")\n",
        "t0 = time.time()\n",
        "train(stud_id=1, path_to_save_model='/content/drive/MyDrive/')\n",
        "t1 = time.time()\n",
        "print(\"training/validation time: {0:.2f}s\".format(t1 - t0))\n",
        "print(\"### DataLoader ready ###\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}